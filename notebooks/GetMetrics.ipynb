{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "04f8bbf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import pickle as pkl\n",
    "import trimesh\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "560fd87e",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_folder = \"/home/wayve/saurabh/pixtrack/ycb_35_round_31/\"\n",
    "mesh_path = \"/mnt/remote/data/prajwal/YCB_Video_Dataset/models/003_cracker_box/textured.obj\"\n",
    "poses_file = os.path.join(result_folder, \"poses.pkl\")\n",
    "with open(poses_file, \"rb\") as f:\n",
    "    poses_file = pkl.load(f)\n",
    "trackers_file = os.path.join(result_folder, \"trackers.pkl\")\n",
    "with open(trackers_file, \"rb\") as f:\n",
    "    trackers_file = pkl.load(f)\n",
    "object_mesh = trimesh.load(mesh_path)\n",
    "vertices = np.array(object_mesh.vertices)\n",
    "vertices = np.hstack((vertices, np.ones((vertices.shape[0], 1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3fed1190",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pose_mat_from_tensor(pose_tensor):\n",
    "    translation = pose_tensor.t.cpu().numpy()\n",
    "    rotation = pose_tensor.R.cpu().numpy()\n",
    "    mesh_pose_in_cam = np.eye(4)\n",
    "    mesh_pose_in_cam[:3, :3] = rotation\n",
    "    mesh_pose_in_cam[:3, -1] = translation\n",
    "    return mesh_pose_in_cam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "1ca4f507",
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity_transform(from_points, to_points):\n",
    "    \n",
    "    assert len(from_points.shape) == 2, \\\n",
    "        \"from_points must be a m x n array\"\n",
    "    assert from_points.shape == to_points.shape, \\\n",
    "        \"from_points and to_points must have the same shape\"\n",
    "    \n",
    "    N, m = from_points.shape\n",
    "    \n",
    "    mean_from = from_points.mean(axis = 0)\n",
    "    mean_to = to_points.mean(axis = 0)\n",
    "    \n",
    "    delta_from = from_points - mean_from # N x m\n",
    "    delta_to = to_points - mean_to       # N x m\n",
    "    \n",
    "    sigma_from = (delta_from * delta_from).sum(axis = 1).mean()\n",
    "    sigma_to = (delta_to * delta_to).sum(axis = 1).mean()\n",
    "    \n",
    "    cov_matrix = delta_to.T.dot(delta_from) / N\n",
    "    \n",
    "    U, d, V_t = np.linalg.svd(cov_matrix, full_matrices = True)\n",
    "    cov_rank = np.linalg.matrix_rank(cov_matrix)\n",
    "    S = np.eye(m)\n",
    "    \n",
    "    if cov_rank >= m - 1 and np.linalg.det(cov_matrix) < 0:\n",
    "        S[m-1, m-1] = -1\n",
    "    elif cov_rank < m-1:\n",
    "        raise ValueError(\"colinearility detected in covariance matrix:\\n{}\".format(cov_matrix))\n",
    "    \n",
    "    R = U.dot(S).dot(V_t)\n",
    "    c = (d * S.diagonal()).sum() / sigma_from\n",
    "    t = mean_to - c*R.dot(mean_from)\n",
    "    \n",
    "    return R, c, t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "4c1cf7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from_trs = []\n",
    "to_trs = []\n",
    "for image_key in poses_file:\n",
    "    to_trs.append(poses_file[image_key][\"T_refined\"].t.cpu().numpy())\n",
    "    from_trs.append(poses_file[image_key][\"gt_pose\"].t.cpu().numpy())\n",
    "R, c, t = similarity_transform(np.array(from_trs), np.array(to_trs))\n",
    "pose_from_res_to_gt = np.eye(4)\n",
    "pose_from_res_to_gt[:3, :3] = R\n",
    "pose_from_res_to_gt[:3, -1] = t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "0616f5c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.transform import Rotation as R\n",
    "from pytorch3d.loss import chamfer_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "044b967d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipped 001104-color.png\n",
      "skipped 001187-color.png\n",
      "Average error distance for all frames: 2.6973091030167073\n",
      "Max error distance for all frames: 6.8041589749815135\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'adds' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[72], line 33\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAverage error distance for all frames:\u001b[39m\u001b[38;5;124m\"\u001b[39m, np\u001b[38;5;241m.\u001b[39mmean(distances)) \n\u001b[1;32m     31\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMax error distance for all frames:\u001b[39m\u001b[38;5;124m\"\u001b[39m, np\u001b[38;5;241m.\u001b[39mmax(distances)) \n\u001b[0;32m---> 33\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mADDS for all frames:\u001b[39m\u001b[38;5;124m\"\u001b[39m, np\u001b[38;5;241m.\u001b[39mmean(\u001b[43madds\u001b[49m))\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28mprint\u001b[39m(np\u001b[38;5;241m.\u001b[39mmax(pose_dists) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m100\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'adds' is not defined"
     ]
    }
   ],
   "source": [
    "distances = []\n",
    "add_ss = []\n",
    "pose_dists = []\n",
    "for image_key in poses_file:\n",
    "    res_pose_mat = get_pose_mat_from_tensor(poses_file[image_key][\"T_refined\"])\n",
    "    gt_pose_mat = get_pose_mat_from_tensor(poses_file[image_key][\"gt_pose\"])\n",
    "    \n",
    "    res_vertices = np.dot(pose_from_res_to_gt, np.dot(res_pose_mat, vertices.T)).T[:, :3] * 100\n",
    "    gt_vertices = np.dot(gt_pose_mat, vertices.T).T[:, :3] * 100\n",
    "    \n",
    "    if (not poses_file[image_key][\"success\"]):\n",
    "        quat1 = R.from_matrix(gt_pose_mat[:3, :3]).as_quat()\n",
    "        quat2 = R.from_matrix(res_pose_mat[:3, :3]).as_quat()\n",
    "        print(f\"skipped {image_key}\")\n",
    "        continue\n",
    "    #     add_s = chamfer_distance(\n",
    "    #         torch.tensor(res_vertices.astype(np.float32)).unsqueeze(0).cuda(), \n",
    "    #         torch.tensor(gt_vertices.astype(np.float32)).unsqueeze(0).cuda(), point_reduction=\"mean\"\n",
    "    #     )[0].cpu().item() \n",
    "    #     add_ss.append(add_s)\n",
    "    l2_distances = np.linalg.norm(gt_vertices - res_vertices, axis=1)\n",
    "    #print(l2_distances)\n",
    "    pose_dist = np.linalg.norm(gt_pose_mat[:3, -1] - res_pose_mat[:3, -1])\n",
    "    pose_dists.append(pose_dist)\n",
    "\n",
    "\n",
    "    l2_dist = np.mean(l2_distances)\n",
    "    distances.append(l2_dist)\n",
    "    \n",
    "print(\"Average error distance for all frames:\", np.mean(distances)) \n",
    "print(\"Max error distance for all frames:\", np.max(distances)) \n",
    "\n",
    "print(\"ADDS for all frames:\", np.mean(adds))\n",
    "print(np.max(pose_dists) * 100)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1981d854",
   "metadata": {},
   "outputs": [],
   "source": [
    "Average error distance for all frames: 2.9226985174149362\n",
    "Max error distance for all frames: 8.428148720680085"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "656ee2a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "poses_file[image_key][\"gt_pose\"].__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "7a1ef092",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.04549179,  0.00412873, -0.0186813 ,  0.01048095],\n",
       "       [-0.00326594,  1.04455582,  0.04807898, -0.05124176],\n",
       "       [ 0.01885129, -0.04801259,  1.04439386, -0.02700372],\n",
       "       [ 0.        ,  0.        ,  0.        ,  1.        ]])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pose_from_res_to_gt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa54a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "trackers_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee88c0d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
